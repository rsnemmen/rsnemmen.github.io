<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://rsnemmen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://rsnemmen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-19T21:11:14+00:00</updated><id>https://rsnemmen.github.io/feed.xml</id><title type="html">Rodrigo Nemmen’s website</title><subtitle>Rodrigo Nemmen&apos;s Data Science and Machine Learning Portfolio. </subtitle><entry><title type="html">Introducing GPUmonty: Simulating Black Hole Light with GPUs</title><link href="https://rsnemmen.github.io/blog/2026/introducing-gpumonty-simulating-black-hole-light-with-gpus/" rel="alternate" type="text/html" title="Introducing GPUmonty: Simulating Black Hole Light with GPUs"/><published>2026-02-16T20:22:10+00:00</published><updated>2026-02-16T20:22:10+00:00</updated><id>https://rsnemmen.github.io/blog/2026/introducing-gpumonty-simulating-black-hole-light-with-gpus</id><content type="html" xml:base="https://rsnemmen.github.io/blog/2026/introducing-gpumonty-simulating-black-hole-light-with-gpus/"><![CDATA[<p>This has been a long time coming, and I am thrilled that we are finally making this codebase public! Our group is releasing <a href="https://github.com/black-hole-group/gpumonty">GPUmonty: a GPU-accelerated Monte Carlo radiative transfer code</a> that simulates the appearance of hot gas falling onto a black hole. The big news is that our code is the first to solve this problem using GPUs, and it is over ten times faster than previous solutions. The entire stack is built on CUDA C.</p> <p>&lt;h2 id=tracking-photons-in-curved-spacetime&gt;Tracking Photons in Curved Spacetime&lt;/h2&gt;&lt;p&gt;Doing this properly is tricky business. First, this is no ordinary ray tracing, because photons are moving through the curved spacetime around a black hole. GPUmonty handles any background metric provided by the user; in black hole physics, it is customary to adopt the Kerr metric, which describes a spinning black hole.&lt;/p&gt;</p> <p>We must then propagate photons from their origin to the observer. To do so, we solve two sets of equations: the geodesic equation, which governs particle motion, and the radiative transfer equation, which determines whether a photon is absorbed or scattered on its way to the observer. The first set is nonlinear, while the second is linear, but each comes with its own challenges. For example, it is currently impossible to solve these equations for every individual photon, so one trick we use is to model families of photons with similar energies called "superphotons."</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/nemmen/output.gif" alt="output"/> <strong>Figure 1:</strong> Sketch of the physics of photon propagation from hot gas around a black hole.</p> <p>Another challenge is modeling the light source: the hot plasma flowing around the event horizon. In reality, we do not know the conditions of such plasma in nature very well, but we have a pretty good idea thanks to intense work by observers and the theorists who model their observations. For example, most supermassive black holes in nearby galaxies are surrounded by tenuous plasma that is incredibly hot, reaching temperatures of about one trillion Kelvin ((\sim 10^{12}) K). This type of plasma model is called a radiatively inefficient accretion flow, or RIAF.</p> <p>Another source of uncertainty is the magnetic topology of the flow — in other words, what is the shape of the magnetic field lines threading the plasma as it moves around the black hole? This is a major topic of debate in the community. So before generating a spectrum, you first have to model the plasma and its magnetic fields, and then you fire up GPUmonty to see what it would look like to an astronomer far away.</p> <p>Once the photons reach the mock camera, we count them and produce a histogram binned by photon energy (we astrophysicists call that a spectrum) that looks like this:</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/nemmen/sane_comparison.webp" alt="sane_comparison"/> <strong>Figure 2:</strong> Electromagnetic spectrum produced by hot gas swirling around a Kerr black hole. One million superphotons were considered here. Parameters chosen for Sagittarius A*, the supermassive black hole at the center of Our Galaxy. The (y)-axis is luminosity; the (x)-axis is frequency. From <a href="https://arxiv.org/abs/2602.13198">Motta et al.</a></p> <p>&lt;h2 id=why-is-this-important&gt;Why Is This Important?&lt;/h2&gt;&lt;p&gt;Sadly — or thankfully, depending on your background — we do not live close to a black hole given our current technology. We cannot simply travel to one and take in situ measurements of the accretion flow and magnetic fields. Instead, we rely on astronomical observations and then solve the inverse problem of extracting parameters from those observations to test our models. Fortunately, we have a wealth of telescopes observing the universe from long radio wavelengths all the way to gamma rays whose wavelengths are smaller than an atom.&lt;/p&gt;</p> <p>This is why codes such as GPUmonty are so important in astrophysics: using these simulations, we can generate synthetic spectra produced by gas as it falls onto a black hole, and then by comparing those models to actual multiwavelength observations of stellar-mass and supermassive black holes, we can learn about the plasma density, temperature, magnetic fields, and even black hole spin.</p> <p>As our observatories improve, the volume of data they generate is growing so rapidly that theorists have to play catch-up. This is strong motivation for faster codes. Another reason for accelerating our codes is that we often need to perform parameter sweeps in which we generate many models across a range of values. GPUmonty will be invaluable on that front as well.</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/nemmen/gpumonty_vs_igrmonty.webp" alt="gpumonty_vs_igrmonty"/> <strong>Figure 3:</strong> How GPUmonty scales with the number of superphotons, compared to a CPU-based solution. We reach up to 12× speedups. From <a href="https://arxiv.org/abs/2602.13198">Motta et al.</a></p> <p>&lt;h2 id=paper-and-code&gt;Paper and Code&lt;/h2&gt;&lt;p&gt;To play with our code, <a href="https://github.com/black-hole-group/gpumonty">visit our GitHub repository</a>. You will need an NVIDIA GPU, a black hole plasma snapshot to serve as the light source, and several libraries, all described in the repo.&lt;/p&gt;</p> <p>Details of the (astro)physics, numerical methods, CUDA kernels, and profiling are available <a href="https://arxiv.org/abs/2602.13198">in the paper we just submitted</a> to <em>The Astrophysical Journal</em>.</p>]]></content><author><name></name></author></entry><entry><title type="html">You are probably overpaying for intelligence</title><link href="https://rsnemmen.github.io/blog/2026/you-are-probably-overpaying-for-intelligence/" rel="alternate" type="text/html" title="You are probably overpaying for intelligence"/><published>2026-02-13T21:01:00+00:00</published><updated>2026-02-13T21:01:00+00:00</updated><id>https://rsnemmen.github.io/blog/2026/you-are-probably-overpaying-for-intelligence</id><content type="html" xml:base="https://rsnemmen.github.io/blog/2026/you-are-probably-overpaying-for-intelligence/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>The leading AI models keep getting more expensive, but are they worth it? I benchmarked dozens of LLMs across coding and general reasoning tasks, then mapped their performance against API pricing. The results are striking: a handful of models—most notably DeepSeek V3.2 and Minimax 2.5—deliver 85–95% of frontier performance at a fraction of the cost. Meanwhile, premium models like Claude 4.6 Opus and GPT-5.2 fight over margins so thin they're often within statistical noise. In this post, I break down the rankings, highlight the best value picks in each category, and argue that for most production workloads, the smartest choice isn't the most expensive — it's the one that gives you the most intelligence per dollar.</p> <p><strong>Update (Feb 18 2026):</strong> Things are evolving so rapidly that this post would need updating every couple of days. For the latest LLM rankings compiled from several benchmarks using the methodology outlined here, <a href="https://rsnemmen.github.io/llm-ranking">visit this page</a>.</p> <h2 id="introduction">Introduction</h2> <p>Large Language Models (LLMs) are incredibly useful, but is it <a href="https://claude.com/pricing/max">necessary</a> to <a href="https://chatgpt.com/pricing">spend</a> <a href="https://one.google.com/">over $100</a> to achieve high-level performance? How do the recent open-weights models released by Chinese companies compare to major players like GPT, Claude, and Gemini?</p> <p>To answer these questions, I conducted a statistical analysis of LLM rankings. My specific goals were:</p> <ol> <li>Derive the average ranking of models from multiple LLM leaderboards.</li> <li>Group models into tiers with similar performance levels.</li> <li>Plot model performance against relative cost to identify which models offer performance comparable to top-tier options at a lower price.</li> </ol> <p>By analyzing the results, we can identify the highest-performing models as well as those offering the optimal cost-benefit ratio.</p> <h2 id="method">Method</h2> <p>&lt;h3 id=benchmark-selection-and-data-collection&gt;Benchmark selection and data collection&lt;/h3&gt;&lt;p&gt;I manually collected model rankings from established LLM leaderboards on February 11, 2026, choosing benchmarks that cover different evaluation angles.&lt;/p&gt;</p> <p><em>General reasoning</em> draws on four leaderboards:</p> <ul> <li><a href="https://livebench.ai">LiveBench</a> — regularly refreshed, multi-domain evaluation designed to resist contamination</li> <li><a href="https://arena.ai/leaderboard">Arena</a> — ELO-style ranking from blind human preference votes (a popular-preference signal rather than a strict accuracy benchmark, but informative nonetheless)</li> <li><a href="https://artificialanalysis.ai">Artificial Analysis Intelligence Index</a> — a composite of 10 evaluations including GPQA Diamond, Humanity's Last Exam, SciCode, and others</li> <li><a href="https://scale.com/leaderboard/humanitys_last_exam">Scale's Humanity's Last Exam</a> — expert-level reasoning questions</li> </ul> <p><em>Coding and agentic coding</em> draws on seven leaderboards:</p> <ul> <li>The three leaderboards previously mentioned (using coding-specific subcategories where applicable) plus four focused evaluations below.</li> <li><a href="https://scale.com/leaderboard/swe_bench_pro_public">Scale’s SWE Bench Pro Public</a></li> <li><a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley Function-Calling Leaderboard (BFCL)</a> — tool and function call accuracy</li> <li><a href="https://www.swebench.com">SWE-bench Verified</a> — real-world GitHub issue resolution</li> <li><a href="https://www.tbench.ai/leaderboard/terminal-bench/2.0">Terminal-Bench 2.0</a> — agentic terminal and shell-based coding tasks</li> </ul> <p>I selected 22 models spanning the current frontier: proprietary leaders from OpenAI (GPT-5.x family), Anthropic (Claude 4.x family), Google (Gemini 3), and xAI (Grok 4.x), plus open-weights challengers from DeepSeek, Moonshot AI (Kimi), Z AI (GLM), MiniMax, Alibaba (Qwen3), and Meta (Llama 4).</p> <p>&lt;h3 id=percentile-normalization&gt;Percentile normalization&lt;/h3&gt;&lt;p&gt;Different leaderboards evaluate different numbers of models. Being ranked 5th out of 600 models is far more impressive than 5th out of 30. To make cross-benchmark comparisons fair, I normalize each rank to a fractional percentile:&lt;/p&gt;</p> <p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mtext>percentile</mtext><mo>&#x0003D;</mo><mfrac><mrow><mtext>rank</mtext></mrow><mrow><mtext>total&#x000A0;models&#x000A0;evaluated</mtext></mrow></mfrac></mrow></math></p> <p>This puts every score on a 0–1 scale (0 = best, 1 = worst). A model that's 3rd out of 600 on Arena (percentile 0.005) and 2nd out of 50 on LiveBench (percentile 0.04) now live on the same axis.</p> <p>&lt;h3 id=aggregation-and-penalties&gt;Aggregation and penalties&lt;/h3&gt;&lt;p&gt;A model’s composite score is the unweighted arithmetic mean of its percentiles across all benchmarks it appears on. To avoid boosting the ranking of a model that happens to score well on the single benchmark it was evaluated on, a sparse-data penalty is added: +0.25 for one benchmark, +0.10 for two, and zero for three or more.&lt;/p&gt; &lt;h3 id=statistical-tiering&gt;Statistical tiering&lt;/h3&gt;&lt;p&gt;Rather than treating every rank difference as meaningful, I group models into <em>tiers</em> using the “indistinguishable from best” method:&lt;/p&gt;</p> <ol> <li>The best model becomes the tier leader.</li> <li>Every remaining model whose <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>&#x000B1;</mi><mn>1</mn><mi>&#x003C3;</mi></mrow></math> confidence interval overlaps with the leader's joins the same tier.</li> <li>Remove the tier, promote the next-best model to leader, and repeat.</li> </ol> <p>Formally, <em>model i</em> is grouped with <em>leader j</em> if:</p> <p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mtext>score</mtext><mi>i</mi></msub><mo>&#x0002B;</mo><msub><mi>&#x003C3;</mi><mi>i</mi></msub><mo>&#x02265;</mo><msub><mtext>score</mtext><mi>j</mi></msub><mo>&#x02212;</mo><msub><mi>&#x003C3;</mi><mi>j</mi></msub></mrow></math></p> <p>In plain terms: if B's best-case performance could plausibly reach A's worst-case, we can't confidently distinguish them. This acknowledges that benchmark scores carry noise and avoids drawing artificial distinctions where the evidence doesn't support them. Models evaluated on fewer than two benchmarks have no standard deviation, so the average σ across all other models is used as a stand-in.</p> <p>&lt;h3 id=cost-metric&gt;Cost metric&lt;/h3&gt;&lt;p&gt;For consistent cost comparison, I use the credit cost per 1,000 tokens from a single API provider (<a href="https://poe.com">Poe</a>) as a uniform pricing reference. Absolute dollar costs vary by provider and plan, but this gives a stable <em>relative</em> comparison across all models on the same platform.&lt;/p&gt;</p> <hr/> <h2 id="results">Results</h2> <p>&lt;h3 id=general-reasoning-a-crowded-tier-1&gt;General reasoning: a crowded Tier 1&lt;/h3&gt;&lt;p&gt;The general reasoning ranking, aggregated across four leaderboards, reveals a tight race at the top—and a perhaps surprising amount of statistical ambiguity below it.&lt;/p&gt;</p> <table> <thead> <tr> <th>Rank</th> <th>Model</th> <th>Avg Pctl</th> <th>Std Dev</th> <th># Benchmarks</th> <th>Cost/1k</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>opus46</td> <td>0.027</td> <td>0.027</td> <td>4</td> <td>850</td> </tr> <tr> <td>2</td> <td>gpt52</td> <td>0.029</td> <td>0.018</td> <td>4</td> <td>470</td> </tr> <tr> <td>3</td> <td>gemini-pro3</td> <td>0.038</td> <td>0.044</td> <td>4</td> <td>370</td> </tr> <tr> <td>4</td> <td>kimi25</td> <td>0.073</td> <td>0.068</td> <td>4</td> <td>120</td> </tr> <tr> <td>5</td> <td>glm5</td> <td>0.076</td> <td>0.077</td> <td>4</td> <td>140</td> </tr> <tr> <td>6</td> <td>grok</td> <td>0.100</td> <td>0.117</td> <td>3</td> <td>600</td> </tr> <tr> <td>7</td> <td>deepseek32</td> <td>0.116</td> <td>0.090</td> <td>4</td> <td>23</td> </tr> <tr> <td>8</td> <td>gemini-flash3</td> <td>0.125</td> <td>0.134</td> <td>4</td> <td>100</td> </tr> <tr> <td>9</td> <td>gpt-codex53</td> <td>0.143</td> <td>0.033</td> <td>2</td> <td>470</td> </tr> <tr> <td>10</td> <td>sonnet45</td> <td>0.156</td> <td>0.135</td> <td>4</td> <td>500</td> </tr> <tr> <td>11</td> <td>grok-fast41</td> <td>0.158</td> <td>0.143</td> <td>3</td> <td>20</td> </tr> <tr> <td>12</td> <td>minimax25</td> <td>0.180</td> <td>0.172</td> <td>4</td> <td>50</td> </tr> <tr> <td>13</td> <td>gpt-pro52</td> <td>0.209</td> <td>0.061</td> <td>2</td> <td>5600</td> </tr> <tr> <td>14</td> <td>qwen3-235b</td> <td>0.216</td> <td>0.221</td> <td>3</td> <td>300</td> </tr> <tr> <td>15</td> <td>qwen3-80b</td> <td>0.254</td> <td>0.235</td> <td>3</td> <td>N/A</td> </tr> <tr> <td>16</td> <td>gpt-oss-20b</td> <td>0.298</td> <td>N/A</td> <td>1</td> <td>15</td> </tr> <tr> <td>17</td> <td>gpt-oss-120b</td> <td>0.376</td> <td>0.294</td> <td>3</td> <td>40</td> </tr> <tr> <td>18</td> <td>gpt-instant52</td> <td>0.383</td> <td>0.208</td> <td>3</td> <td>470</td> </tr> <tr> <td>19</td> <td>llama-maverick</td> <td>0.395</td> <td>0.242</td> <td>2</td> <td>55</td> </tr> <tr> <td>20</td> <td>haiku45</td> <td>0.580</td> <td>0.293</td> <td>2</td> <td>170</td> </tr> <tr> <td>21</td> <td>qwen3-32b</td> <td>0.636</td> <td>0.276</td> <td>2</td> <td>N/A</td> </tr> <tr> <td>22</td> <td>qwen3-coder-30b</td> <td>0.720</td> <td>0.173</td> <td>2</td> <td>50</td> </tr> </tbody> </table> <p><em>The top three are virtually tied</em>. Opus 4.6, GPT-5.2, and Gemini 3 Pro are in the top 4% of every leaderboard. The gap separating first from third is roughly one percentile point.</p> <p>But the real headline is how wide Tier 1 is. The statistical tiering places the majority of models—12 of 22—in the top tier. The reason: with only four benchmarks, many models exhibit high cross-benchmark variance. A model might rank 2nd on Arena but 15th on Humanity's Last Exam. The resulting confidence intervals are wide enough to overlap with the leader's. The practical implication is that <em>for general reasoning, the measurable performance gap between the best model and the 12th-best model is smaller than the noise in the measurements.</em> When you use these models, you can clearly notice the superiority in performance between the top model (Opus 4.6) ones and say the 7th model (DeepSeek 3.2). But this is what the statistics is telling us.</p> <p><em>When performance is a wash, cost becomes the story</em>. The plot below makes this vivid:</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/residuals/ranks_general.webp" alt="ranks_general"/> <strong>Figure 1:</strong> <em>General intelligence ranking</em>. Credit cost (log scale) on the x-axis and average percentile score on the y-axis (inverted, so the best models appear at the top). Error bars show ±1σ. Colors indicate statistical tier. Circles mark proprietary models; squares mark open-weights models. The upper-left corner is the sweet spot: best performance at lowest cost.</p> <p>The entire Tier 1 cluster (blue) spans from 20 credits (Grok 4.1 Fast) to 850 credits (Claude Opus 4.6)—a <em>42× cost difference</em>—while delivering statistically indistinguishable average performance. Meanwhile, GPT-5.2 Pro at 5,600 credits per 1k tokens (the rightmost point) sits in Tier 2, spending 243× more than the cheapest Tier 1 model for measurably worse average results. Its extended reasoning budget may help on specific hard reasoning tasks, but that advantage is diluted when averaged across diverse evaluations.</p> <p><strong>The “general reasoning” value picks:</strong></p> <table> <thead> <tr> <th>Model</th> <th>General rank</th> <th>Cost</th> <th>Note</th> </tr> </thead> <tbody> <tr> <td>Kimi K2.5</td> <td>4</td> <td>120</td> <td>Top 5 at 7× less than Opus</td> </tr> <tr> <td>GLM-5</td> <td>5</td> <td>140</td> <td>Matches Kimi at similar price; open weights</td> </tr> <tr> <td>DeepSeek V3.2</td> <td>7</td> <td>23</td> <td>Tier 1 performance at 37× less than Opus</td> </tr> <tr> <td>Grok 4.1 Fast</td> <td>11</td> <td>20</td> <td>Cheapest Tier 1 model; high variance though (σ = 0.143)</td> </tr> <tr> <td>MiniMax M2.5</td> <td>12</td> <td>50</td> <td>Solid mid-tier at 17× less than Opus</td> </tr> <tr> <td>Gemini 3 Pro</td> <td>3</td> <td>370</td> <td>If you want a top-3 model, it's the cheapest of the three</td> </tr> </tbody> </table> <p>&lt;h3 id=coding-and-agentic-coding-opus-stands-out&gt;Coding and agentic coding: Opus stands out&lt;/h3&gt;&lt;p&gt;The coding ranking, drawn from seven benchmarks including demanding agentic evaluations, tells a more subtle story.&lt;/p&gt;</p> <table> <thead> <tr> <th>Rank</th> <th>Model</th> <th>Avg Pctl</th> <th>Std Dev</th> <th># Benchmarks</th> <th>Cost/1k</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>opus46</td> <td>0.017</td> <td>0.016</td> <td>7</td> <td>850</td> </tr> <tr> <td>2</td> <td>gpt-codex53</td> <td>0.038</td> <td>0.027</td> <td>6</td> <td>470</td> </tr> <tr> <td>3</td> <td>gemini-pro3</td> <td>0.043</td> <td>0.033</td> <td>7</td> <td>370</td> </tr> <tr> <td>4</td> <td>gpt52</td> <td>0.065</td> <td>0.056</td> <td>7</td> <td>470</td> </tr> <tr> <td>5</td> <td>gpt-pro52</td> <td>0.096</td> <td>0.132</td> <td>3</td> <td>5600</td> </tr> <tr> <td>6</td> <td>gemini-flash3</td> <td>0.103</td> <td>0.089</td> <td>7</td> <td>100</td> </tr> <tr> <td>7</td> <td>sonnet45</td> <td>0.113</td> <td>0.135</td> <td>7</td> <td>500</td> </tr> <tr> <td>8</td> <td>kimi25</td> <td>0.128</td> <td>0.136</td> <td>7</td> <td>120</td> </tr> <tr> <td>9</td> <td>glm5</td> <td>0.130</td> <td>0.201</td> <td>7</td> <td>140</td> </tr> <tr> <td>10</td> <td>minimax25</td> <td>0.155</td> <td>0.162</td> <td>6</td> <td>50</td> </tr> <tr> <td>11</td> <td>deepseek32</td> <td>0.237</td> <td>0.222</td> <td>7</td> <td>23</td> </tr> <tr> <td>12</td> <td>haiku45</td> <td>0.240</td> <td>0.204</td> <td>5</td> <td>170</td> </tr> <tr> <td>13</td> <td>grok</td> <td>0.282</td> <td>0.288</td> <td>5</td> <td>600</td> </tr> <tr> <td>14</td> <td>grok-fast41</td> <td>0.301</td> <td>0.300</td> <td>5</td> <td>20</td> </tr> <tr> <td>15</td> <td>llama-maverick</td> <td>0.306</td> <td>0.257</td> <td>4</td> <td>55</td> </tr> <tr> <td>16</td> <td>gpt-instant52</td> <td>0.321</td> <td>0.292</td> <td>5</td> <td>470</td> </tr> <tr> <td>17</td> <td>gpt-oss-120b</td> <td>0.344</td> <td>0.282</td> <td>5</td> <td>40</td> </tr> <tr> <td>18</td> <td>mistral</td> <td>0.347</td> <td>N/A</td> <td>1</td> <td>400</td> </tr> <tr> <td>19</td> <td>qwen3-80b</td> <td>0.350</td> <td>0.346</td> <td>3</td> <td>N/A</td> </tr> <tr> <td>20</td> <td>qwen3-235b</td> <td>0.375</td> <td>0.262</td> <td>6</td> <td>300</td> </tr> <tr> <td>21</td> <td>qwen3-coder-30</td> <td>0.460</td> <td>0.348</td> <td>5</td> <td>50</td> </tr> <tr> <td>22</td> <td>gpt-oss-20b</td> <td>0.489</td> <td>0.355</td> <td>3</td> <td>15</td> </tr> </tbody> </table> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/residuals/ranks_coding.webp" alt="ranks_coding"/> <strong>Figure 2:</strong> <em>Coding and agentic coding ranking</em>. Notation is the same as in Figure 1.</p> <p><em>Claude Opus 4.6 is the clear coding champion</em>. It consistently lands in the top 3% across all seven coding benchmarks—a level of reliability no other model matches. GPT-5.3 Codex and Gemini 3 Pro are also strong, but notice how Opus's standard deviation is roughly half of theirs.</p> <p><em>Agentic benchmarks expose inconsistencies</em>. Several models that score well on traditional coding tasks (function calling, SWE-bench Verified) stumble on agentic evaluations like Terminal-Bench 2.0 and SWE-Bench Pro, where the model must navigate multi-step tool use and environment interaction. Several models (e.g. GLM-5 with <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>&#x003C3;</mi><mo>&#x0003D;</mo><mn>0.2</mn></mrow></math>, Grok with <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>&#x003C3;</mi><mo>&#x02248;</mo><mn>0.3</mn></mrow></math>) show particularly high variance—their best and worst benchmark performances differ by an order of magnitude in percentile terms. Pure code generation ability doesn't reliably predict agentic coding performance.</p> <p><strong>The “coding” value picks:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Coding rank</th> <th>Cost</th> <th>Note</th> </tr> </thead> <tbody> <tr> <td>Kimi K2.5</td> <td>8</td> <td>120</td> <td>Consistently solid across all 7 benchmarks</td> </tr> <tr> <td>MiniMax M2.5</td> <td>10</td> <td>50</td> <td>17× cheaper than Opus</td> </tr> <tr> <td>Gemini 3 Flash</td> <td>6</td> <td>100</td> <td></td> </tr> <tr> <td>DeepSeek V3.2</td> <td>11</td> <td>23</td> <td>Competitive at a tiny fraction of the cost</td> </tr> </tbody> </table> <p>&lt;h3 id=other-observations&gt;Other observations&lt;/h3&gt;&lt;p&gt;<strong>Consistency across the two rankings.</strong> Claude Opus 4.6 is #1 in both. GPT-5.2 is #2 in general but #4 in coding (the specialized Codex variant takes #2 there). Gemini 3 Pro is #3 in both—arguably the most balanced model overall. The biggest mover is Grok 4, which lands at #6 in general reasoning but drops to #13 in coding, suggesting its strengths are more on the reasoning side than the code generation side.&lt;/p&gt;</p> <p><strong>The Chinese open-weights models are the story of this analysis.</strong> Four models—Kimi K2.5, GLM-5, DeepSeek V3.2, and MiniMax M2.5—land in the top 12 for <em>both</em> general reasoning and coding. Their average cost is approximately 83 credits per 1k tokens, roughly <em>10× less</em> than the Big Three models (Opus, GPT-5.2, Gemini 3 Pro) average of ~560 credits. They are especially compelling if you can tolerate somewhat higher variance—they're brilliant on some benchmarks, merely good on others—compared to the more uniform excellence of the priciest proprietary models.</p> <p>The notable exception among Chinese models is Qwen3: the 235B variant ranks 14th in general and 20th in coding; the smaller Coder 30B model is near the bottom of both lists.</p> <p><strong>Cost scales much faster than quality.</strong> Going from DeepSeek V3.2 (23 credits) to Opus 4.6 (850 credits) is a 37× price increase that buys roughly a 4× improvement in average general reasoning percentile (0.116 → 0.027). Spending further to reach GPT-5.2 Pro (5,600 credits)—another 6.6× cost increase—actually makes things <em>worse</em> on average (0.209 percentile). The diminishing returns curve is brutally steep at the high end.</p> <h2 id="conclusions">Conclusions</h2> <ul> <li>If you need the absolute best coding reliability and can afford it, Claude Opus 4.6 earns its price.</li> <li>For general reasoning, you have several options: Kimi K2.5 or GLM-5 deliver Tier 1 performance at a fraction of the cost.</li> <li>For coding and agentic coding (e.g. <a href="https://opencode.ai">opencode</a>), there are many options with great value: Gemini Flash 3, Kimi K2.5, GLM-5 and MiniMax M2.5.</li> <li>If budget is the primary concern, DeepSeek V3.2 at 23 credits per 1k tokens is the standout value of this generation—statistically indistinguishable from the leaders on general reasoning, and still in the top half for coding, at roughly the cost of a rounding error.</li> </ul> <hr/> <h2 id="reproducibility">Reproducibility</h2> <p>All code, data files, and full methodology details are <a href="https://github.com/rsnemmen/rank-clippies">here</a>. You can reproduce the rankings with</p> <div class="highlight"><pre><span></span><span class="c1"># General ranking</span>
<span class="n">python</span> <span class="n">rank_models</span><span class="o">.</span><span class="n">py</span> <span class="n">ranks_general</span><span class="o">.</span><span class="n">txt</span> <span class="o">--</span><span class="n">plot</span>
<span class="c1"># Coding ranking</span>
<span class="n">python</span> <span class="n">rank_models</span><span class="o">.</span><span class="n">py</span> <span class="n">ranks_coding</span><span class="o">.</span><span class="n">txt</span> <span class="o">--</span><span class="n">plot</span>
</pre></div> <p>These results correspond to <a href="https://github.com/rsnemmen/rank-clippies/commit/423486a1e0328d58d2aeaebabc2405835908431e">commit <code>423486a1e0328</code></a>.</p> <h2 id="changelog">Changelog</h2> <p>Feb 13 2026: Light introduction revision (phrasing).<br/> Feb 14 2026: Minor formatting fixes.<br/> Feb 15: I realized that the https://artificialanalysis.ai index shows only a selection of models in their plots, so their ranking is a bit misleading. I fixed this and as a result the average rankings and tiers were slightly impacted. <em>Conclusions are unchanged</em>. For the latest plots, please see [this page]. Feb 18 2026: Added note pointing to updated website.</p>]]></content><author><name></name></author></entry><entry><title type="html">pip-select, an interactive tool for updating pip-installed python packages (conda-friendly)</title><link href="https://rsnemmen.github.io/blog/2026/pip-select-an-interactive-tool-for-updating-pip-installed-python-packages-conda-friendly/" rel="alternate" type="text/html" title="pip-select, an interactive tool for updating pip-installed python packages (conda-friendly)"/><published>2026-02-01T07:22:00+00:00</published><updated>2026-02-01T07:22:00+00:00</updated><id>https://rsnemmen.github.io/blog/2026/pip-select-an-interactive-tool-for-updating-pip-installed-python-packages-conda-friendly</id><content type="html" xml:base="https://rsnemmen.github.io/blog/2026/pip-select-an-interactive-tool-for-updating-pip-installed-python-packages-conda-friendly/"><![CDATA[<p>I was tired of manually figuring out which of my pip-installed packages need to be updated while excluding the conda ones, so I just created a tool that takes care of this. You fire <code>pip-select</code>, and it will check which of your pip packages can be updated while excluding the ones managed by conda, with a nice progress bar:</p> <div class="highlight"><pre><span></span>Conda environment detected at: /home/user/miniconda3/envs/myenv
Detected 84 pip-installed packages (excluded 141 conda-installed).
Checking 84 packages [████████████████████████████░░] 87%   
</pre></div> <p>It will present a menu where you can select which packages to update:</p> <div class="highlight"><pre><span></span>SPACE=toggle  ↑/↓/PgUp/PgDn=move  Home/End=jump  a=all  n=none  Enter=upg  q=quit  Selected: 3/84
[ ] aiohttp                         3.13.2       -&gt; 3.13.3      
[ ] beartype                        0.14.1       -&gt; 0.22.9      
[x] datasets                        4.4.2        -&gt; 4.5.0       
[ ] dill                            0.4.0        -&gt; 0.4.1       
[ ] dol                             0.3.37       -&gt; 0.3.38      
[x] numpy                           1.24.0       -&gt; 2.0.0       
[x] pandas                          2.0.0        -&gt; 2.2.0       
[ ] requests                        2.28.0       -&gt; 2.31.0      
</pre></div> <p>And that's it. You can keep updating your conda packages as you usually do. <code>pip-select</code> will take care of the pip ones.</p> <p><a href="https://github.com/rsnemmen/pip-select">You can find <code>pip-select</code> here</a>.</p> <p>If you find this interesting, beta-testers are welcome. Just be sure to duplicate your conda environment before using pip-select.</p>]]></content><author><name></name></author></entry><entry><title type="html">The state of agentic coding</title><link href="https://rsnemmen.github.io/blog/2026/the-state-of-agentic-coding/" rel="alternate" type="text/html" title="The state of agentic coding"/><published>2026-01-25T22:19:00+00:00</published><updated>2026-01-25T22:19:00+00:00</updated><id>https://rsnemmen.github.io/blog/2026/the-state-of-agentic-coding</id><content type="html" xml:base="https://rsnemmen.github.io/blog/2026/the-state-of-agentic-coding/"><![CDATA[<p>With the <a href="https://www.nytimes.com/2026/01/23/technology/claude-code.html">explosion</a> of <a href="https://code.claude.com/docs/en/overview">Claude Code</a>, developers are struggling with handle the credit limits. I have seen plenty of Reddit posts complaining about the limitations imposed by the Claude Max plan, even though it costs a whopping $200!</p> <p>I access Anthropic’s models using the <a href="https://poe.com/login">Poe API service</a>, which also gives me access to models by the other major players in the field. In my experience with CC so far, I can easily burn a few percent of the monthly credits in a few minutes of use in a complex codebase—even if you stick with Sonnet, Anthropic’s mid-tier clippy. I can totally see my monthly API credit (about $30) vanishing within one or two days of intense CC sessions in a complex scientific codebase.</p> <p>I am not spending a couple hundred dollars on clippies, so I have been exploring alternative ways of doing agentic coding and alternative providers. In my explorations, I found <a href="https://opencode.ai/">opencode</a>: another fantastic terminal-based agentic coding interface which gives you access to other models in addition to Claude. <sup class="footnote-ref" id="fnref-1"><a href="#fn-1">1</a></sup> More importantly, it gives me access to a locally-hosted LLM (more about that in another post).</p> <p>In order to see what else is out there in terms of agentic-coding LLMs and rank their cost-benefit, I crawled the major relevant benchmarks out there. I computed the average ranking of major models, including how much they cost. I went through the following benchmarks in no particular order:</p> <ul> <li><a href="https://gorilla.cs.berkeley.edu/leaderboard.html#leaderboard">Berkeley Function-Calling Leaderboard</a></li> <li><a href="https://www.swebench.com">SWE-bench</a></li> <li><a href="https://livebench.ai/#/">LiveBench</a></li> <li><a href="https://lmarena.ai/leaderboard">LMArena</a> (with the usual disclaimer that this is based on benchmarks weighted by popular votes, so it is really a popular preference ranking; still useful though)</li> <li><a href="https://scale.com/leaderboard/swe_bench_pro_public">Scale’s SWE-Bench Pro (Public Dataset)</a></li> </ul> <p>Here is the ranking:</p> <table> <thead> <tr> <th>Ranking</th> <th>Model Name</th> <th>Average Score</th> <th>Credit Cost (per 1k)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>opus</td> <td>1.00</td> <td>850</td> </tr> <tr> <td>2</td> <td>gemini</td> <td>2.75</td> <td>370</td> </tr> <tr> <td>3</td> <td>gpt</td> <td>5.25</td> <td>470</td> </tr> <tr> <td>4</td> <td>gpt-pro</td> <td>5.50</td> <td>5600</td> </tr> <tr> <td>5</td> <td>sonnet</td> <td>6.25</td> <td>500</td> </tr> <tr> <td>6</td> <td>minimax</td> <td>6.67</td> <td>50</td> </tr> <tr> <td>7</td> <td>gpt-codex</td> <td>10.00</td> <td>340</td> </tr> <tr> <td>8</td> <td>gemini-flash</td> <td>12.00</td> <td>100</td> </tr> <tr> <td>9</td> <td>deepseek-32</td> <td>13.67</td> <td>23</td> </tr> <tr> <td>10</td> <td>kimi</td> <td>14.00</td> <td>225</td> </tr> <tr> <td>11</td> <td>glm</td> <td>14.33</td> <td>94</td> </tr> <tr> <td>12</td> <td>haiku</td> <td>15.00</td> <td>170</td> </tr> <tr> <td>13</td> <td>devstral-small</td> <td>24.50</td> <td>1</td> </tr> <tr> <td>14</td> <td>gpt-instant</td> <td>26.67</td> <td>470</td> </tr> <tr> <td>15</td> <td>qwen3-80b</td> <td>28.33</td> <td>300</td> </tr> <tr> <td>16</td> <td>grok-fast</td> <td>31.00</td> <td>20</td> </tr> <tr> <td>17</td> <td>grok</td> <td>31.50</td> <td>600</td> </tr> <tr> <td>18</td> <td>qwen3-coder-30b</td> <td>32.67</td> <td>50</td> </tr> <tr> <td>19</td> <td>devstral2</td> <td>72.00*</td> <td>N/A</td> </tr> <tr> <td>20</td> <td>mistral</td> <td>79.00*</td> <td>400</td> </tr> <tr> <td>21</td> <td>llama-maverick</td> <td>80.00*</td> <td>55</td> </tr> <tr> <td>22</td> <td>qwen3-235b</td> <td>88.00*</td> <td>N/A</td> </tr> </tbody> </table> <p><em>Table showing the LLM ranking for agentic coding. When a model does not include the version, it refers to the latest version available (i.e. <code>gemini</code> means Gemini 3 Pro High, <code>gpt</code> means GPT 5.2 High). Scores with an asterisk include a +50 penalty for incomplete information (only 1 ranking source). Credit cost is the relative cost per 1000 tokens of input and output.</em></p> <p>Here is a plot showing the model's score as a function of how much it cost per 1k tokens.</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/residuals/19pm.webp" alt="Screenshot 2026-01-25 at 2"/></p> <p>It comes as no surprise that Opus 4.5, Gemini 3 Pro and GPT 5.2 are among the top 3. But notice how open models are getting quite competitive: Minimax M2.1, DeepSeek v3.2 and Kimi K2—all open models which anyone can download—are among the top 10. Also notice how Minimax offers coding performance as good as Sonnet 4.5 but costing 10 times less.</p> <p>My take from this? Whenever possible I will stick with opencode and Minimax for day-to-day coding tasks. When I need the big guns, I will use CC/Sonnet/Opus.</p> <hr/> <section class="footnotes"> <ol> <li id="fn-1"><p>Recently Anthropic devs enabled CC to talk to other clippies besides Claude. My experience so far with CC 2.1.15 is that it does not work very well and CC crashes as soon as you try to make it talk to a locally-hosted Ollama model.<a href="#fnref-1" class="footnote">&#8617;</a></p></li> </ol> </section>]]></content><author><name></name></author></entry><entry><title type="html">Nature News &amp;amp; Views article</title><link href="https://rsnemmen.github.io/blog/2026/nature-news-views-article/" rel="alternate" type="text/html" title="Nature News &amp;amp; Views article"/><published>2026-01-14T16:55:44+00:00</published><updated>2026-01-14T16:55:44+00:00</updated><id>https://rsnemmen.github.io/blog/2026/nature-news--views-article</id><content type="html" xml:base="https://rsnemmen.github.io/blog/2026/nature-news-views-article/"><![CDATA[<p>Nature asked me to write a <a href="https://rdcu.be/eY4PS">News &amp; Views article</a> on a <a href="https://www.nature.com/articles/s41586-025-09900-4">recent hot result</a> on little red dots. N&amp;V articles explain the results of new research paper in a way that is accessible to a broad community of scientists outside the specific field, while giving the author’s opinion on the work. LRDs are the hottest thing in astronomy right now, and nobody really knows their nature (no pun intended). As you will learn in <a href="https://rdcu.be/eY4PS">my N&amp;V piece</a>, the astronomical community is converging on a “hey, actually they are growing massive black holes” kind of model.</p> <p>I hope you enjoy my article.</p> <h2 id="acknowledgements">Acknowledgements</h2> <p>It can be tricky writing to a broad community of scientists. I would like to thank the following colleagues who gave me feedback on the first draft of the N&amp;V piece: Rafael de Souza (fast responder!), Paula Coelho, Matt Luzum, Gastão Lima Neto, Nina Hirata, Thaisa Storchi Bergmann, Rafael Ribeiro and Raniere de Menezes.</p>]]></content><author><name></name></author></entry><entry><title type="html">From horizon scales to cocoons: My favorite black hole papers of 2025</title><link href="https://rsnemmen.github.io/blog/2026/from-horizon-scales-to-cocoons-my-favorite-black-hole-papers-of-2025/" rel="alternate" type="text/html" title="From horizon scales to cocoons: My favorite black hole papers of 2025"/><published>2026-01-08T22:19:00+00:00</published><updated>2026-01-08T22:19:00+00:00</updated><id>https://rsnemmen.github.io/blog/2026/from-horizon-scales-to-cocoons-my-favorite-black-hole-papers-of-2025</id><content type="html" xml:base="https://rsnemmen.github.io/blog/2026/from-horizon-scales-to-cocoons-my-favorite-black-hole-papers-of-2025/"><![CDATA[<p>New year, new science. Time to think about my favorite papers of 2025. This is of course a hugely biased list, convolved with my personal scientific preferences. This list will be focused on the topics of black hole astrophysics.</p> <p>&lt;h2 id=phenomenology-agns-little-red-dots&gt;Phenomenology: AGNs, little red dots&lt;/h2&gt;&lt;p&gt;Not a terribly exciting year on the observational front for AGNs and black hole binaries. If you discount little red dots, that is.&lt;/p&gt;</p> <p>There were the <a href="https://www.aanda.org/articles/aa/full_html/2025/12/aa55855-25/aa55855-25.html">polarization flips observed in M87* on horizon scales</a>. Really hard work, and in my opinion polarization offers the biggest science lessons from EHT. No surprises in the polarization properties mapped. All variability is consistent with accretion physics expectations (good!).</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/residuals/resampled_triptych_eht_labeled_1.webp" alt="resampled_triptych_EHT_labeled_1"/> <em>Prettified polarization maps on horizon scales (by prettified I mean line integral convolution images, looks good!). From <a href="https://www.aanda.org/articles/aa/full_html/2025/12/aa55855-25/aa55855-25.html">EHTC (2025)</a>.</em></p> <p>For me, the biggest puzzle in polarization studies is the following. Sgr A* and M87* have basically the same polarization properties. The same GRMHD models explain the observations: a MAD around a rapidly rotating black hole with <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mi>a</mi><mo>&#x0002A;</mo></msub><mo>&#x02248;</mo><mn>0.9</mn></mrow></math>. <em>So why the heck does M87 produce huge jets while Sgr A shows none of them?</em>. If you feed such a Kerr black hole with lots of magnetic fields, you should get strong jets. What is special about Sgr A* then? And no, I do not believe in the evidence so far for jets from Sgr A*.</p> <p>The hottest thing in the observational front right now—and honestly in all fields of astronomy—is the <em>little red dot</em> phenomenon. This is a transformational field where we know very little about what is behind those sources. Comparable to when quasars were discovered, or gamma-ray bursts. It is breath-taking and hard to catch up with so many papers being posted (about 200 since LRDs were discovered two years ago).</p> <p>The top-2 results on LRDs this year were:</p> <p>(1) <a href="https://arxiv.org/abs/2503.16595">High S/N JWST spectra of LRDs are better explained by exponential line profiles</a>. This has huge implications because the line broadening we are seeing would be mostly explained by Thomson scattering in an optically thick, highly-ionized medium, not virial motion around a central mass. This is important evidence that LRDs are growing massive black holes enshrouded in a cocoon—which some authors are calling a <a href="https://arxiv.org/abs/2503.16596">“black hole star”</a> (I hate that term, confusing on so many levels)—and they are not overmassive with respect to their galaxies as many authors think.</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/residuals/06pm.webp" alt="Screenshot 2026-01-08 at 3"/> <em>Exponential fit to one of the high S/N little red dot line profiles compiled performed by <a href="https://arxiv.org/abs/2503.16595">Rusakov+2025</a>. In my opinion, the money plot of this paper is in the supplementary material—Extended Figure 7, which shows how statistically superior the exponential fits are, compared to Gaussians.</em></p> <p>(2) <a href="https://arxiv.org/abs/2412.04224v2">LRDs don’t produce radio emission</a>. If they host accreting black holes, that is weird. We should see at least some of them with some radio emission. But then, if (1) above is correct, it would explain why we should not expect so much radio from these guys.</p> <p>I think we can pretty much say that LRDs are not just galaxies. So we can move on and focus on the black hole explanation.</p> <p>&lt;h2 id=theory-tilted-disks-kinetic-multizone-simulations&gt;Theory: tilted disks, kinetic, multizone simulations&lt;/h2&gt;&lt;p&gt;There has been amazing progress in the theoretical front, particularly in multiscale GRMHD and kinetic simulations. Part of that is due to new algorithms being implemented, particularly in the kinetic case. Who would say that we would be talking about GR-kinetic simulations in the 2020s? But most of the progress is happening, I think, because Moore’s law is allowing us to solve equations with dense meshes that were unthinkable a couple of years ago. Yay, NVIDIA.&lt;/p&gt;</p> <p>There has been important progress in <a href="https://iopscience.iop.org/article/10.3847/1538-4357/adce79">simulating spark gaps in Kerr black hole magnetospheres</a>. We need much more of this if we want to understand how plasma is maintained near event horizons, and how are jets produced (Blandford-Znajek is only part of the story). This is a regime where GRMHD can only take you so far.</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/residuals/25pm.webp" alt="Screenshot 2026-01-08 at 3"/> <em>Snapshots showing cross-sectional cuts of several variables from a 2D GR-kinetic simulation. Dashed line is the ergosphere. From <a href="https://iopscience.iop.org/article/10.3847/1538-4357/adce79">Yuan+2025</a>.</em></p> <p>We have the <a href="https://arxiv.org/abs/2311.00432">impressive work of the <code>H-AMR</code> folks</a> systematically exploring disk tilts and seeing what comes out of it.</p> <p>And finally, we have the <a href="https://arxiv.org/abs/2507.17818v1">multizone GRMHD simulations bridging the gap from horizon scales up to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>&#x0007E;</mi><mn>100</mn></mrow></math> Bondi radii</a>! Incredible work based on <a href="https://iopscience.iop.org/article/10.1088/0004-637X/761/2/130">a simple and powerful insight on how to connect boundary conditions</a> by Feng Yuan and collaborators.</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/residuals/23pm.webp" alt="Screenshot 2026-01-08 at 3"/> <em>Multizone GRMHD simulation: Feeding and feedback from jets and winds out to <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mn>10</mn><mn>7</mn></msup><msub><mi>r</mi><mi>g</mi></msub></mrow></math> (kpc-scale for <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msup><mn>10</mn><mn>8</mn></msup><msub><mi>M</mi><mo>&#x02299;</mo></msub></mrow></math>). From <a href="https://arxiv.org/abs/2507.17818v1">Cho+2025</a>.</em></p> <p>Excited to see what 2026 will bring.</p> <p>--</p> <h2 id="changelog">Changelog</h2> <ul> <li>v1, 1-8-2025, 1pm: published post</li> <li>v2, 1-8-2025, 4pm: added figures</li> </ul>]]></content><author><name></name></author></entry><entry><title type="html">AGN jets review talk at Zihuatanejo</title><link href="https://rsnemmen.github.io/blog/2025/agn-jets-review-talk-at-zihuatanejo/" rel="alternate" type="text/html" title="AGN jets review talk at Zihuatanejo"/><published>2025-12-17T04:12:52+00:00</published><updated>2025-12-17T04:12:52+00:00</updated><id>https://rsnemmen.github.io/blog/2025/agn-jets-review-talk-at-zihuatanejo</id><content type="html" xml:base="https://rsnemmen.github.io/blog/2025/agn-jets-review-talk-at-zihuatanejo/"><![CDATA[<p>I was invited to give a review talk about relativistic jets from AGN in the <a href="https://zihuagn.irya.unam.mx">”AGN redemption" conference</a>. The venue was at beautiful, humid Zihuatanejo. I haven't been in such a excruciatingly hot place since I lived in Porto Alegre, or when I visited Rio during summer. So if you are wondering if Zihuatanejo is warm at the end of October: yes sir! Very much so.</p> <p>Anyway, the organizers asked me to give a comprehensive overview (30 minutes) of "jet formation and evolution". The theme of the conference was the <a href="https://www.imdb.com/title/tt0111161/?ref_=nv_sr_srsg_0_tt_8_nm_0_in_0_q_shawshank">Shawshank Redemption</a> movie, because this is the place where the movie ends (if you haven't watched the movie, go stream it). So the organizers made a big deal about it. They asked the presenters to mention their favorite movie at the end of each talk. Most of them did, but I chose not to because I did not have 1 minute to spare in my review talk (reviewing the entire field of AGN jets in 30 minutes is no easy task, it turns out).</p> <p>I decided to pick about ten papers in the field that caught my attention in the last 2-3 years, and focused on those papers and why they are relevant. And I strictly concentrated on formation, dynamics/kinematics and the status of the radio-loud/radio-quiet division. The slides are <a href="https://doi.org/10.6084/m9.figshare.30899891">available here</a>. For future reference, the talk was given on October 31st 2025.</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/nemmen/03pm.webp" alt="Screenshot 2025-12-16 at 8"/></p> <p>By the way, I should point out a correction to the slide 27 about the second M87* ring: even though I wrote in the slide that it was observed by EHT, it was actually seen by ALMA+GLT+GMVA. My bad.</p>]]></content><author><name></name></author></entry><entry><title type="html">Fuzzy file operations with fuzzycp</title><link href="https://rsnemmen.github.io/blog/2025/fuzzy_file_operations/" rel="alternate" type="text/html" title="Fuzzy file operations with fuzzycp"/><published>2025-07-02T23:00:00+00:00</published><updated>2025-07-02T23:00:00+00:00</updated><id>https://rsnemmen.github.io/blog/2025/fuzzy_file_operations</id><content type="html" xml:base="https://rsnemmen.github.io/blog/2025/fuzzy_file_operations/"><![CDATA[<p>I just created a new program that performs file operations with fuzzy filename matching: <a href="https://github.com/rsnemmen/fuzzy_cp">fuzzycp</a>. In order to understand what that means, here is a concrete example.</p> <p>Suppose you have a file <code class="language-plaintext highlighter-rouge">names.txt</code> containing a list of names you want to match against. Let’s say the content of this file is:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. The Legend of Zelda: Ocarina of Time
2. Super Mario 64
3. Mario Kart 64
4. GoldenEye 007
5. Super Smash Bros.
</code></pre></div></div> <p>This is a random example—the top five games released for the Nintendo 64 console. Now, you have a directory with thousands of files, and you want to copy to another directory only the files that are the best-match to the names in the above list. Here are some examples of files in that directory:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'Spider-Man (U) [!].v64'
'StarCraft 64 (U) [!].v64'
'Starfox 64 1.1 (U).v64'
'Starshot - Space Circus Fever (U) [!].z64'
'Star Wars - Rogue Squadron (U) [!].v64'
'Star Wars - Shadows of the Empire (U) (V1.2) [!].v64'
'Star Wars Episode I - Battle for Naboo (U) [!].v64'
'Star Wars Episode I - Racer (U) [!].v64'
'Stunt Racer 64 (U) [!].z64'
'Super Bowling 64 (U) [!].z64'
'Supercross 2000 (U) [!].z64'
'Superman (U) (M3) [!].z64'
'Super Mario 64 (U) [!].v64'
</code></pre></div></div> <p>For the sake of our example, the content of these files is meaningless (let’s say they have the metadata for those games). Notice that there will be no exact match between the names in <code class="language-plaintext highlighter-rouge">names.txt</code> and the actual filenames. They could have different casing, missing text, extra letters etc. This is where the power of fuzzy matching shines: you don’t need an exact match.</p> <h2 id="how-fuzzycp-solves-this-problem">How fuzzycp solves this problem</h2> <p>Normally people would do this sort of thing by manually selecting file by file and copying them. Not anymore. Here is how you solve this using <code class="language-plaintext highlighter-rouge">fuzzycp</code>, doing in a few seconds what would take potentially hours:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>fuzzycp names.txt -c dest/directory
</code></pre></div></div> <p>After asking for the user’s confirmation and showing a list of the bestCopy only the best-matching files to directory <code class="language-plaintext highlighter-rouge">dest/directory</code>:</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fuzzycp-480.webp 480w,/assets/img/fuzzycp-800.webp 800w,/assets/img/fuzzycp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/fuzzycp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>Internally, fuzzycp compares the names using the <code class="language-plaintext highlighter-rouge">QRatio</code>(Quick Ratio) scorer, which uses a simple Levenshtein-based percentage after basic lowercase/whitespace cleaning. This is the fastest scorer in RapidFuzz, useful for quick filters or typo-level comparisons. More sophisticated scorer can easily be implemented.</p> <h2 id="get-fuzzycp-today">Get fuzzycp today!</h2> <p>fuzzycp is <a href="https://github.com/rsnemmen/fuzzy_cp">available on Github</a>.</p>]]></content><author><name></name></author><category term="software"/><category term="projects"/><category term="software"/><summary type="html"><![CDATA[I just created a new program that performs file operations with fuzzy filename matching: fuzzycp. In order to understand what that means, here is a concrete example.]]></summary></entry><entry><title type="html">Model deployment with FastAPI and Docker</title><link href="https://rsnemmen.github.io/blog/2025/deploy/" rel="alternate" type="text/html" title="Model deployment with FastAPI and Docker"/><published>2025-05-01T22:00:00+00:00</published><updated>2025-05-01T22:00:00+00:00</updated><id>https://rsnemmen.github.io/blog/2025/deploy</id><content type="html" xml:base="https://rsnemmen.github.io/blog/2025/deploy/"><![CDATA[<p>The ability to operationalize a machine-learning model is just as important as the model itself. In <a href="https://rsnemmen.github.io/projects/2_deploy/">this project</a> I demonstrate how a trained XGBoost classifier is (1) served via an API with FastAPI and (2) containerized with Docker and (3) deployed to Google Cloud Platform (Cloud Run).</p>]]></content><author><name></name></author><category term="data-science"/><category term="data-science"/><category term="projects"/><summary type="html"><![CDATA[The ability to operationalize a machine-learning model is just as important as the model itself. In this project I demonstrate how a trained XGBoost classifier is (1) served via an API with FastAPI and (2) containerized with Docker and (3) deployed to Google Cloud Platform (Cloud Run).]]></summary></entry></feed>