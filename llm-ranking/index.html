<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LLM ranking | Rodrigo Nemmen's website </title> <meta name="author" content="Rodrigo Nemmen"> <meta name="description" content="LLM ranking including proprietary and open models"> <meta name="keywords" content="data-science, machine-learning, mle, portfolio, ds, ml"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8C%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://rsnemmen.github.io/llm-ranking/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/css/lightbox.min.css" integrity="sha256-uypRbsAiJcFInM/ndyI/JHpzNe6DtUNXaWEUWEPfMGo=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Rodrigo Nemmen's website </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item dropdown active"> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">projects <span class="sr-only">(current)</span> </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/projects/">all projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item active" href="/llm-ranking/">LLM ranking</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">research </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/astrophysics/">astrophysics</a> <a class="dropdown-item " href="/key-science/">key discoveries</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/applied/">applied research</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">LLM ranking</h1> <p class="post-description">LLM ranking including proprietary and open models</p> </header> <article> <p>I benchmarked dozens of LLMs across coding and general reasoning tasks, then mapped their performance against API pricing. This page breaks down the rankings and highlight the best value picks in each category.</p> <p>Updated on Feb. 19 2026.</p> <h2 id="introduction">Introduction</h2> <p>I conducted a statistical analysis of LLM rankings. My specific goals were:</p> <ol> <li>Derive the average ranking of models from multiple LLM leaderboards.</li> <li>Group models into tiers with similar performance levels.</li> <li>Plot model performance against relative cost to identify which models offer performance comparable to top-tier options at a lower price.</li> </ol> <p>By analyzing the results, we can identify the highest-performing models as well as those offering the optimal cost-benefit ratio.</p> <h2 id="results">Results</h2> <h3 id="general-reasoning">General reasoning</h3> <p>The general reasoning ranking, aggregated across four leaderboards, reveals a tight race at the top—and a perhaps surprising amount of statistical ambiguity below it.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+------------------------------------------------------------------------------------+
| Rank  | Model                   | Avg Pctl  | Std Dev   | # Benchmarks  | Cost/1k  |
+------------------------------------------------------------------------------------+
| 1     | opus46                  | 0.015     | 0.008     | 4             | 850      |
| 2     | gpt52                   | 0.069     | 0.046     | 4             | 470      |
| 3     | gemini-pro3             | 0.080     | 0.044     | 4             | 370      |
| 4     | sonnet46                | 0.124     | 0.079     | 4             | 510      |
| 5     | glm5                    | 0.144     | 0.095     | 4             | 140      |
| 6     | kimi25                  | 0.167     | 0.085     | 4             | 120      |
| 7     | gpt-codex53             | 0.179     | 0.005     | 2             | 470      |
| 8     | qwen3-max               | 0.220     | 0.089     | 3             | 220      |
| 9     | grok41                  | 0.233     | 0.146     | 4             | 600      |
| 10    | grok-fast41             | 0.243     | 0.116     | 3             | 24       |
| 11    | gpt-pro52               | 0.244     | 0.022     | 2             | 5700     |
| 12    | gemini-flash3           | 0.261     | 0.291     | 4             | 100      |
| 13    | deepseek32              | 0.261     | 0.105     | 4             | 23       |
| 14    | haiku45                 | 0.290     | 0.076     | 3             | 170      |
| 15    | qwen35                  | 0.343     | 0.074     | 2             | 86       |
| 16    | minimax25               | 0.358     | 0.128     | 4             | 50       |
| 17    | qwen3-235b              | 0.395     | 0.190     | 3             | 300      |
| 18    | qwen3-80b               | 0.462     | 0.178     | 3             | 80       |
| 19    | glm-flash47             | 0.497     | 0.146     | 3             | 17       |
| 20    | gpt-instant52           | 0.523     | 0.289     | 4             | 470      |
| 21    | gpt-oss-120b            | 0.552     | 0.184     | 4             | 40       |
| 22    | qwen3-32b               | 0.699     | 0.181     | 4             | 40       |
| 23    | llama-maverick          | 0.709     | 0.187     | 3             | 55       |
| 24    | gpt-oss-20b             | 0.793     | 0.087     | 2             | 15       |
| 25    | nemotron-nano3          | 0.856     | N/A       | 1             | 20       |
| 26    | qwen3-coder-30b         | 0.904     | N/A       | 1             | 50       |
+------------------------------------------------------------------------------------+
</code></pre></div></div> <p><a href="/assets/img/clippies/ranks_general_ranking.png" data-lightbox="roadtrip"> <img src="/assets/img/clippies/ranks_general_ranking.png" alt="General Ranking" style="width: 45%;"> </a> <a href="/assets/img/clippies/ranks_general.png" data-lightbox="roadtrip"> <img src="/assets/img/clippies/ranks_general.png" alt="General Scores" style="width: 45%;"> </a></p> <p><strong>Figure 1. Left panel:</strong> Overall ranking. Circle size proportional to log10(API cost). <strong>Right panel:</strong> Credit cost (log scale) on the x-axis and average percentile score on the y-axis (inverted, so the best models appear at the top). The upper-left corner is the sweet spot: best performance at lowest cost. In both panels, error bars show ±1σ. Colors indicate statistical tier. Circles mark proprietary models; squares mark open-weights models.</p> <h3 id="coding-and-agentic-coding">Coding and agentic coding</h3> <p>The coding ranking, drawn from seven benchmarks including demanding agentic evaluations, tells a more subtle story.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>+------------------------------------------------------------------------------------+
| Rank  | Model                   | Avg Pctl  | Std Dev   | # Benchmarks  | Cost/1k  |
+------------------------------------------------------------------------------------+
| 1     | opus46                  | 0.025     | 0.030     | 7             | 850      |
| 2     | sonnet46                | 0.065     | 0.039     | 7             | 510      |
| 3     | gpt-codex53             | 0.094     | 0.090     | 6             | 470      |
| 4     | gpt52                   | 0.117     | 0.072     | 7             | 470      |
| 5     | gemini-pro3             | 0.121     | 0.133     | 7             | 370      |
| 6     | kimi25                  | 0.154     | 0.106     | 7             | 120      |
| 7     | glm5                    | 0.165     | 0.187     | 7             | 140      |
| 8     | qwen35                  | 0.189     | 0.105     | 3             | 86       |
| 9     | gemini-flash3           | 0.207     | 0.142     | 7             | 100      |
| 10    | gpt-pro52               | 0.239     | 0.139     | 2             | 5700     |
| 11    | minimax25               | 0.284     | 0.256     | 6             | 50       |
| 12    | deepseek32              | 0.299     | 0.171     | 7             | 23       |
| 13    | grok-fast41             | 0.336     | 0.276     | 4             | 20       |
| 14    | mistral                 | 0.345     | N/A       | 1             | 400      |
| 15    | haiku45                 | 0.364     | 0.257     | 7             | 170      |
| 16    | glm-flash47             | 0.386     | 0.135     | 3             | 17       |
| 17    | qwen3-max               | 0.407     | 0.252     | 3             | 220      |
| 18    | grok41                  | 0.464     | 0.290     | 6             | 600      |
| 19    | qwen3-235b              | 0.473     | 0.312     | 6             | 300      |
| 20    | gpt-instant52           | 0.582     | 0.413     | 6             | 470      |
| 21    | qwen3-coder-30b         | 0.636     | 0.420     | 4             | 50       |
| 22    | gpt-oss-120b            | 0.673     | 0.180     | 5             | 40       |
| 23    | qwen3-80b               | 0.707     | 0.397     | 3             | 80       |
| 24    | llama-maverick          | 0.770     | 0.421     | 5             | 55       |
| 25    | gpt-oss-20b             | 0.781     | 0.241     | 4             | 15       |
| 26    | qwen3-32b               | 0.983     | 0.267     | 3             | 40       |
| 27    | nemotron-nano3          | 1.000     | N/A       | 1             | 20       |
+------------------------------------------------------------------------------------+
</code></pre></div></div> <p><a href="/assets/img/clippies/ranks_coding_ranking.png" data-lightbox="roadtrip"> <img src="/assets/img/clippies/ranks_coding_ranking.png" alt="Coding Ranking" style="width: 45%;"> </a> <a href="/assets/img/clippies/ranks_coding.png" data-lightbox="roadtrip"> <img src="/assets/img/clippies/ranks_coding.png" alt="Coding Scores" style="width: 45%;"> </a></p> <p><strong>Figure 2:</strong> Same conventions as Figure 1.</p> <h2 id="methods">Methods</h2> <h3 id="benchmark-selection-and-data-collection">Benchmark selection and data collection</h3> <p>I manually collected model rankings from established LLM leaderboards on February 11, 2026, choosing benchmarks that cover different evaluation angles.</p> <p><em>General reasoning</em> draws on four leaderboards:</p> <ul> <li> <a href="https://livebench.ai" rel="external nofollow noopener" target="_blank">LiveBench</a> — regularly refreshed, multi-domain evaluation designed to resist contamination</li> <li> <a href="https://arena.ai/leaderboard" rel="external nofollow noopener" target="_blank">Arena</a> — ELO-style ranking from blind human preference votes (a popular-preference signal rather than a strict accuracy benchmark, but informative nonetheless)</li> <li> <a href="https://artificialanalysis.ai" rel="external nofollow noopener" target="_blank">Artificial Analysis Intelligence Index</a> — a composite of 10 evaluations including GPQA Diamond, Humanity’s Last Exam, SciCode, and others</li> <li> <a href="https://scale.com/leaderboard/humanitys_last_exam" rel="external nofollow noopener" target="_blank">Scale’s Humanity’s Last Exam</a> — expert-level reasoning questions</li> </ul> <p><em>Coding and agentic coding</em> draws on seven leaderboards:</p> <ul> <li>The three leaderboards previously mentioned (using coding-specific subcategories where applicable) plus four focused evaluations below.</li> <li><a href="https://scale.com/leaderboard/swe_bench_pro_public" rel="external nofollow noopener" target="_blank">Scale’s SWE Bench Pro Public</a></li> <li> <a href="https://gorilla.cs.berkeley.edu/leaderboard.html" rel="external nofollow noopener" target="_blank">Berkeley Function-Calling Leaderboard (BFCL)</a> — tool and function call accuracy</li> <li> <a href="https://www.swebench.com" rel="external nofollow noopener" target="_blank">SWE-bench Verified</a> — real-world GitHub issue resolution</li> <li> <a href="https://www.tbench.ai/leaderboard/terminal-bench/2.0" rel="external nofollow noopener" target="_blank">Terminal-Bench 2.0</a> — agentic terminal and shell-based coding tasks</li> </ul> <p>I selected 20+ models spanning the current frontier: proprietary leaders from OpenAI (GPT-5.2+ family), Anthropic (Claude 4.5+ family), Google (Gemini 3), and xAI (Grok 4+), plus open-weights challengers from DeepSeek, Moonshot AI (Kimi), Z AI (GLM), MiniMax, Alibaba (Qwen3+), and Meta/NVIDIA (Llama 4).</p> <h3 id="percentile-normalization">Percentile normalization</h3> <p>Different leaderboards evaluate different numbers of models. Being ranked 5th out of 600 models is far more impressive than 5th out of 30. To make cross-benchmark comparisons fair, I normalize each rank to a fractional percentile:</p> \[\text{percentile} = \frac{\text{rank}}{\text{total models evaluated}}\] <p>This puts every score on a 0–1 scale (0 = best, 1 = worst). A model that’s 3rd out of 600 on Arena (percentile 0.005) and 2nd out of 50 on LiveBench (percentile 0.04) now live on the same axis.</p> <h3 id="aggregation-and-penalties">Aggregation and penalties</h3> <p>A model’s composite score is the unweighted arithmetic mean of its percentiles across all benchmarks it appears on. To avoid boosting the ranking of a model that happens to score well on the single benchmark it was evaluated on, a sparse-data penalty is added: +0.25 for one benchmark, +0.10 for two, and zero for three or more.</p> <h3 id="statistical-tiering">Statistical tiering</h3> <p>Rather than treating every rank difference as meaningful, I group models into <em>tiers</em> using the “indistinguishable from best” method:</p> <ol> <li>The best model becomes the tier leader.</li> <li>Every remaining model whose \(\pm 1\sigma\) confidence interval overlaps with the leader’s joins the same tier.</li> <li>Remove the tier, promote the next-best model to leader, and repeat.</li> </ol> <p>Formally, <em>model i</em> is grouped with <em>leader j</em> if:</p> \[\text{score}_i + \sigma_i \geq \text{score}_j - \sigma_j\] <p>In plain terms: if B’s best-case performance could plausibly reach A’s worst-case, we can’t confidently distinguish them. This acknowledges that benchmark scores carry noise and avoids drawing artificial distinctions where the evidence doesn’t support them. Models evaluated on fewer than two benchmarks have no standard deviation, so the average σ across all other models is used as a stand-in.</p> <h3 id="cost-metric">Cost metric</h3> <p>For consistent cost comparison, I use the credit cost per 1,000 tokens from a single API provider (<a href="https://poe.com" rel="external nofollow noopener" target="_blank">Poe</a>) as a uniform pricing reference. Absolute dollar costs vary by provider and plan, but this gives a stable <em>relative</em> comparison across all models on the same platform.</p> <h2 id="conclusions">Conclusions</h2> <ul> <li>If you need the absolute best coding reliability and can afford it, Claude Opus 4.6 earns its price. It ranks consistently at tier 1.</li> <li>For general reasoning, you have several affordable options: Kimi K2.5, GLM-5 deliver Tier 2 performance at a fraction of the cost. Also competitive: Gemini Flash 3 and Qwen 3 Max.</li> <li>For coding and agentic coding (e.g. <a href="https://opencode.ai" rel="external nofollow noopener" target="_blank">opencode</a>), there are many options with great value: Gemini Flash 3, Kimi K2.5, GLM-5, MiniMax M2.5 and Qwen 3.5.</li> <li>If budget is the primary concern, DeepSeek V3.2 and Grok 4.1 Fast offer the standout value of this generation.</li> </ul> <hr> <h2 id="reproducibility">Reproducibility</h2> <p>All code, data files, and full methodology details are <a href="https://github.com/rsnemmen/rank-clippies" rel="external nofollow noopener" target="_blank">here</a>.</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Rodrigo Nemmen. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/lightbox2@2.11.5/dist/js/lightbox.min.js" integrity="sha256-A6jI5V9s1JznkWwsBaRK8kSeXLgIqQfxfnvdDOZEURY=" crossorigin="anonymous"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>