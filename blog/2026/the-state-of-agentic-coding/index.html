<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>With the <a href="https://www.nytimes.com/2026/01/23/technology/claude-code.html" rel="external nofollow noopener" target="_blank">explosion</a> of <a href="https://code.claude.com/docs/en/overview" rel="external nofollow noopener" target="_blank">Claude Code</a>, developers are struggling with handle the credit limits. I have seen plenty of Reddit posts complaining about the limitations imposed by the Claude Max plan, even though it costs a whopping $200!</p> <p>I access Anthropic’s models using the <a href="https://poe.com/login" rel="external nofollow noopener" target="_blank">Poe API service</a>, which also gives me access to models by the other major players in the field. In my experience with CC so far, I can easily burn a few percent of the monthly credits in a few minutes of use in a complex codebase—even if you stick with Sonnet, Anthropic’s mid-tier clippy. I can totally see my monthly API credit (about $30) vanishing within one or two days of intense CC sessions in a complex scientific codebase.</p> <p>I am not spending a couple hundred dollars on clippies, so I have been exploring alternative ways of doing agentic coding and alternative providers. In my explorations, I found <a href="https://opencode.ai/" rel="external nofollow noopener" target="_blank">opencode</a>: another fantastic terminal-based agentic coding interface which gives you access to other models in addition to Claude. <sup class="footnote-ref" id="fnref-1"><a href="#fn-1">1</a></sup> More importantly, it gives me access to a locally-hosted LLM (more about that in another post).</p> <p>In order to see what else is out there in terms of agentic-coding LLMs and rank their cost-benefit, I crawled the major relevant benchmarks out there. I computed the average ranking of major models, including how much they cost. I went through the following benchmarks in no particular order:</p> <ul> <li><a href="https://gorilla.cs.berkeley.edu/leaderboard.html#leaderboard" rel="external nofollow noopener" target="_blank">Berkeley Function-Calling Leaderboard</a></li> <li><a href="https://www.swebench.com" rel="external nofollow noopener" target="_blank">SWE-bench</a></li> <li><a href="https://livebench.ai/#/" rel="external nofollow noopener" target="_blank">LiveBench</a></li> <li> <a href="https://lmarena.ai/leaderboard" rel="external nofollow noopener" target="_blank">LMArena</a> (with the usual disclaimer that this is based on benchmarks weighted by popular votes, so it is really a popular preference ranking; still useful though)</li> <li><a href="https://scale.com/leaderboard/swe_bench_pro_public" rel="external nofollow noopener" target="_blank">Scale’s SWE-Bench Pro (Public Dataset)</a></li> </ul> <p>Here is the ranking:</p> <table> <thead> <tr> <th>Ranking</th> <th>Model Name</th> <th>Average Score</th> <th>Credit Cost (per 1k)</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>opus</td> <td>1.00</td> <td>850</td> </tr> <tr> <td>2</td> <td>gemini</td> <td>2.75</td> <td>370</td> </tr> <tr> <td>3</td> <td>gpt</td> <td>5.25</td> <td>470</td> </tr> <tr> <td>4</td> <td>gpt-pro</td> <td>5.50</td> <td>5600</td> </tr> <tr> <td>5</td> <td>sonnet</td> <td>6.25</td> <td>500</td> </tr> <tr> <td>6</td> <td>minimax</td> <td>6.67</td> <td>50</td> </tr> <tr> <td>7</td> <td>gpt-codex</td> <td>10.00</td> <td>340</td> </tr> <tr> <td>8</td> <td>gemini-flash</td> <td>12.00</td> <td>100</td> </tr> <tr> <td>9</td> <td>deepseek-32</td> <td>13.67</td> <td>23</td> </tr> <tr> <td>10</td> <td>kimi</td> <td>14.00</td> <td>225</td> </tr> <tr> <td>11</td> <td>glm</td> <td>14.33</td> <td>94</td> </tr> <tr> <td>12</td> <td>haiku</td> <td>15.00</td> <td>170</td> </tr> <tr> <td>13</td> <td>devstral-small</td> <td>24.50</td> <td>1</td> </tr> <tr> <td>14</td> <td>gpt-instant</td> <td>26.67</td> <td>470</td> </tr> <tr> <td>15</td> <td>qwen3-80b</td> <td>28.33</td> <td>300</td> </tr> <tr> <td>16</td> <td>grok-fast</td> <td>31.00</td> <td>20</td> </tr> <tr> <td>17</td> <td>grok</td> <td>31.50</td> <td>600</td> </tr> <tr> <td>18</td> <td>qwen3-coder-30b</td> <td>32.67</td> <td>50</td> </tr> <tr> <td>19</td> <td>devstral2</td> <td>72.00*</td> <td>N/A</td> </tr> <tr> <td>20</td> <td>mistral</td> <td>79.00*</td> <td>400</td> </tr> <tr> <td>21</td> <td>llama-maverick</td> <td>80.00*</td> <td>55</td> </tr> <tr> <td>22</td> <td>qwen3-235b</td> <td>88.00*</td> <td>N/A</td> </tr> </tbody> </table> <p><em>Table showing the LLM ranking for agentic coding. When a model does not include the version, it refers to the latest version available (i.e. <code>gemini</code> means Gemini 3 Pro High, <code>gpt</code> means GPT 5.2 High). Scores with an asterisk include a +50 penalty for incomplete information (only 1 ranking source). Credit cost is the relative cost per 1000 tokens of input and output.</em></p> <p>Here is a plot showing the model's score as a function of how much it cost per 1k tokens.</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/residuals/19pm.webp" alt="Screenshot 2026-01-25 at 2"></p> <p>It comes as no surprise that Opus 4.5, Gemini 3 Pro and GPT 5.2 are among the top 3. But notice how open models are getting quite competitive: Minimax M2.1, DeepSeek v3.2 and Kimi K2—all open models which anyone can download—are among the top 10. Also notice how Minimax offers coding performance as good as Sonnet 4.5 but costing 10 times less.</p> <p>My take from this? Whenever possible I will stick with opencode and Minimax for day-to-day coding tasks. When I need the big guns, I will use CC/Sonnet/Opus.</p> <hr> <section class="footnotes"> <ol> <li id="fn-1"><p>Recently Anthropic devs enabled CC to talk to other clippies besides Claude. My experience so far with CC 2.1.15 is that it does not work very well and CC crashes as soon as you try to make it talk to a locally-hosted Ollama model.<a href="#fnref-1" class="footnote">↩</a></p></li> </ol> </section> </body></html>