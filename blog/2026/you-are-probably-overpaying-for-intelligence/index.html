<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <h2 id="abstract">Abstract</h2> <p>The leading AI models keep getting more expensive, but are they worth it? I benchmarked dozens of LLMs across coding and general reasoning tasks, then mapped their performance against API pricing. The results are striking: a handful of models—most notably DeepSeek V3.2 and Minimax 2.5—deliver 85–95% of frontier performance at a fraction of the cost. Meanwhile, premium models like Claude 4.6 Opus and GPT-5 Pro fight over margins so thin they're often within statistical noise. In this post, I break down the rankings, highlight the best value picks in each category, and argue that for most production workloads, the smartest choice isn't the most expensive — it's the one that gives you the most intelligence per dollar.</p> <h2 id="introduction">Introduction</h2> <p>Large Language Models (LLMs) are incredibly useful, but is it <a href="https://claude.com/pricing/max" rel="external nofollow noopener" target="_blank">necessary</a> to <a href="https://chatgpt.com/pricing" rel="external nofollow noopener" target="_blank">spend</a> <a href="https://one.google.com/" rel="external nofollow noopener" target="_blank">over $100</a> to achieve high-level performance? How do the recent open-weights models released by Chinese companies compare to major players like GPT, Claude, and Gemini?</p> <p>To answer these questions, I conducted a statistical analysis of LLM rankings. My specific goals were:</p> <ol> <li>Derive the average ranking of models from multiple LLM leaderboards.</li> <li>Group models into tiers with similar performance levels.</li> <li>Plot model performance against relative cost to identify which models offer performance comparable to top-tier options at a lower price.</li> </ol> <p>By analyzing the results, we can identify the highest-performing models as well as those offering the optimal cost-benefit ratio.</p> <h2 id="method">Method</h2> <p>&lt;h3 id=benchmark-selection-and-data-collection&gt;Benchmark selection and data collection&lt;/h3&gt;&lt;p&gt;I manually collected model rankings from established LLM leaderboards on February 11, 2026, choosing benchmarks that cover different evaluation angles.&lt;/p&gt;</p> <p><em>General reasoning</em> draws on four leaderboards:</p> <ul> <li> <a href="https://livebench.ai" rel="external nofollow noopener" target="_blank">LiveBench</a> — regularly refreshed, multi-domain evaluation designed to resist contamination</li> <li> <a href="https://arena.ai/leaderboard" rel="external nofollow noopener" target="_blank">Arena</a> — ELO-style ranking from blind human preference votes (a popular-preference signal rather than a strict accuracy benchmark, but informative nonetheless)</li> <li> <a href="https://artificialanalysis.ai" rel="external nofollow noopener" target="_blank">Artificial Analysis Intelligence Index</a> — a composite of 10 evaluations including GPQA Diamond, Humanity's Last Exam, SciCode, and others</li> <li> <a href="https://scale.com/leaderboard/humanitys_last_exam" rel="external nofollow noopener" target="_blank">Scale's Humanity's Last Exam</a> — expert-level reasoning questions</li> </ul> <p><em>Coding and agentic coding</em> draws on seven leaderboards:</p> <ul> <li>The three leaderboards previously mentioned (using coding-specific subcategories where applicable) plus four focused evaluations below.</li> <li><a href="https://scale.com/leaderboard/swe_bench_pro_public" rel="external nofollow noopener" target="_blank">Scale’s SWE Bench Pro Public</a></li> <li> <a href="https://gorilla.cs.berkeley.edu/leaderboard.html" rel="external nofollow noopener" target="_blank">Berkeley Function-Calling Leaderboard (BFCL)</a> — tool and function call accuracy</li> <li> <a href="https://www.swebench.com" rel="external nofollow noopener" target="_blank">SWE-bench Verified</a> — real-world GitHub issue resolution</li> <li> <a href="https://www.tbench.ai/leaderboard/terminal-bench/2.0" rel="external nofollow noopener" target="_blank">Terminal-Bench 2.0</a> — agentic terminal and shell-based coding tasks</li> </ul> <p>I selected 22 models spanning the current frontier: proprietary leaders from OpenAI (GPT-5.x family), Anthropic (Claude 4.x family), Google (Gemini 3), and xAI (Grok 4.x), plus open-weights challengers from DeepSeek, Moonshot AI (Kimi), Z AI (GLM), MiniMax, Alibaba (Qwen3), and Meta (Llama 4).</p> <p>&lt;h3 id=percentile-normalization&gt;Percentile normalization&lt;/h3&gt;&lt;p&gt;Different leaderboards evaluate different numbers of models. Being ranked 5th out of 600 models is far more impressive than 5th out of 30. To make cross-benchmark comparisons fair, I normalize each rank to a fractional percentile:&lt;/p&gt;</p> <p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mtext>percentile</mtext><mo>=</mo><mfrac><mrow><mtext>rank</mtext></mrow><mrow><mtext>total models evaluated</mtext></mrow></mfrac></mrow></math></p> <p>This puts every score on a 0–1 scale (0 = best, 1 = worst). A model that's 3rd out of 600 on Arena (percentile 0.005) and 2nd out of 50 on LiveBench (percentile 0.04) now live on the same axis.</p> <p>&lt;h3 id=aggregation-and-penalties&gt;Aggregation and penalties&lt;/h3&gt;&lt;p&gt;A model’s composite score is the unweighted arithmetic mean of its percentiles across all benchmarks it appears on. To avoid boosting the ranking of a model that happens to score well on the single benchmark it was evaluated on, a sparse-data penalty is added: +0.25 for one benchmark, +0.10 for two, and zero for three or more.&lt;/p&gt; &lt;h3 id=statistical-tiering&gt;Statistical tiering&lt;/h3&gt;&lt;p&gt;Rather than treating every rank difference as meaningful, I group models into <em>tiers</em> using the “indistinguishable from best” method:&lt;/p&gt;</p> <ol> <li>The best model becomes the tier leader.</li> <li>Every remaining model whose <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>±</mi><mn>1</mn><mi>σ</mi></mrow></math> confidence interval overlaps with the leader's joins the same tier.</li> <li>Remove the tier, promote the next-best model to leader, and repeat.</li> </ol> <p>Formally, <em>model i</em> is grouped with <em>leader j</em> if:</p> <p><math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><msub><mtext>score</mtext><mi>i</mi></msub><mo>+</mo><msub><mi>σ</mi><mi>i</mi></msub><mo>≥</mo><msub><mtext>score</mtext><mi>j</mi></msub><mo>−</mo><msub><mi>σ</mi><mi>j</mi></msub></mrow></math></p> <p>In plain terms: if B's best-case performance could plausibly reach A's worst-case, we can't confidently distinguish them. This acknowledges that benchmark scores carry noise and avoids drawing artificial distinctions where the evidence doesn't support them. Models evaluated on fewer than two benchmarks have no standard deviation, so the average σ across all other models is used as a stand-in.</p> <p>&lt;h3 id=cost-metric&gt;Cost metric&lt;/h3&gt;&lt;p&gt;For consistent cost comparison, I use the credit cost per 1,000 tokens from a single API provider (<a href="https://poe.com" rel="external nofollow noopener" target="_blank">Poe</a>) as a uniform pricing reference. Absolute dollar costs vary by provider and plan, but this gives a stable <em>relative</em> comparison across all models on the same platform.&lt;/p&gt;</p> <hr> <h2 id="results">Results</h2> <p>&lt;h3 id=general-reasoning-a-crowded-tier-1&gt;General reasoning: a crowded Tier 1&lt;/h3&gt;&lt;p&gt;The general reasoning ranking, aggregated across four leaderboards, reveals a tight race at the top—and a perhaps surprising amount of statistical ambiguity below it.&lt;/p&gt;</p> <table> <thead> <tr> <th>Rank</th> <th>Model</th> <th>Avg Pctl</th> <th>Std Dev</th> <th># Benchmarks</th> <th>Cost/1k</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>opus46</td> <td>0.027</td> <td>0.027</td> <td>4</td> <td>850</td> </tr> <tr> <td>2</td> <td>gpt52</td> <td>0.029</td> <td>0.018</td> <td>4</td> <td>470</td> </tr> <tr> <td>3</td> <td>gemini-pro3</td> <td>0.038</td> <td>0.044</td> <td>4</td> <td>370</td> </tr> <tr> <td>4</td> <td>kimi25</td> <td>0.073</td> <td>0.068</td> <td>4</td> <td>120</td> </tr> <tr> <td>5</td> <td>glm5</td> <td>0.076</td> <td>0.077</td> <td>4</td> <td>140</td> </tr> <tr> <td>6</td> <td>grok</td> <td>0.100</td> <td>0.117</td> <td>3</td> <td>600</td> </tr> <tr> <td>7</td> <td>deepseek32</td> <td>0.116</td> <td>0.090</td> <td>4</td> <td>23</td> </tr> <tr> <td>8</td> <td>gemini-flash3</td> <td>0.125</td> <td>0.134</td> <td>4</td> <td>100</td> </tr> <tr> <td>9</td> <td>gpt-codex53</td> <td>0.143</td> <td>0.033</td> <td>2</td> <td>470</td> </tr> <tr> <td>10</td> <td>sonnet45</td> <td>0.156</td> <td>0.135</td> <td>4</td> <td>500</td> </tr> <tr> <td>11</td> <td>grok-fast41</td> <td>0.158</td> <td>0.143</td> <td>3</td> <td>20</td> </tr> <tr> <td>12</td> <td>minimax25</td> <td>0.180</td> <td>0.172</td> <td>4</td> <td>50</td> </tr> <tr> <td>13</td> <td>gpt-pro52</td> <td>0.209</td> <td>0.061</td> <td>2</td> <td>5600</td> </tr> <tr> <td>14</td> <td>qwen3-235b</td> <td>0.216</td> <td>0.221</td> <td>3</td> <td>300</td> </tr> <tr> <td>15</td> <td>qwen3-80b</td> <td>0.254</td> <td>0.235</td> <td>3</td> <td>N/A</td> </tr> <tr> <td>16</td> <td>gpt-oss-20b</td> <td>0.298</td> <td>N/A</td> <td>1</td> <td>15</td> </tr> <tr> <td>17</td> <td>gpt-oss-120b</td> <td>0.376</td> <td>0.294</td> <td>3</td> <td>40</td> </tr> <tr> <td>18</td> <td>gpt-instant52</td> <td>0.383</td> <td>0.208</td> <td>3</td> <td>470</td> </tr> <tr> <td>19</td> <td>llama-maverick</td> <td>0.395</td> <td>0.242</td> <td>2</td> <td>55</td> </tr> <tr> <td>20</td> <td>haiku45</td> <td>0.580</td> <td>0.293</td> <td>2</td> <td>170</td> </tr> <tr> <td>21</td> <td>qwen3-32b</td> <td>0.636</td> <td>0.276</td> <td>2</td> <td>N/A</td> </tr> <tr> <td>22</td> <td>qwen3-coder-30b</td> <td>0.720</td> <td>0.173</td> <td>2</td> <td>50</td> </tr> </tbody> </table> <p><em>The top three are virtually tied</em>. Opus 4.6, GPT-5.2, and Gemini 3 Pro are in the top 4% of every leaderboard. The gap separating first from third is roughly one percentile point.</p> <p>But the real headline is how wide Tier 1 is. The statistical tiering places the majority of models—12 of 22—in the top tier. The reason: with only four benchmarks, many models exhibit high cross-benchmark variance. A model might rank 2nd on Arena but 15th on Humanity's Last Exam. The resulting confidence intervals are wide enough to overlap with the leader's. The practical implication is that <em>for general reasoning, the measurable performance gap between the best model and the 12th-best model is smaller than the noise in the measurements.</em> When you use these models, you can clearly notice the superiority in performance between the top model (Opus 4.6) ones and say the 7th model (DeepSeek 3.2). But this is what the statistics is telling us.</p> <p><em>When performance is a wash, cost becomes the story</em>. The plot below makes this vivid:</p> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/residuals/ranks_general.webp" alt="ranks_general"> <strong>Figure 1:</strong> <em>General intelligence ranking</em>. Credit cost (log scale) on the x-axis and average percentile score on the y-axis (inverted, so the best models appear at the top). Error bars show ±1σ. Colors indicate statistical tier. Circles mark proprietary models; squares mark open-weights models. The upper-left corner is the sweet spot: best performance at lowest cost.</p> <p>The entire Tier 1 cluster (blue) spans from 20 credits (Grok 4.1 Fast) to 850 credits (Claude Opus 4.6)—a <em>42× cost difference</em>—while delivering statistically indistinguishable average performance. Meanwhile, GPT-5.2 Pro at 5,600 credits per 1k tokens (the rightmost point) sits in Tier 2, spending 243× more than the cheapest Tier 1 model for measurably worse average results. Its extended reasoning budget may help on specific hard reasoning tasks, but that advantage is diluted when averaged across diverse evaluations.</p> <p><strong>The “general reasoning” value picks:</strong></p> <table> <thead> <tr> <th>Model</th> <th>General rank</th> <th>Cost</th> <th>Note</th> </tr> </thead> <tbody> <tr> <td>Kimi K2.5</td> <td>4</td> <td>120</td> <td>Top 5 at 7× less than Opus</td> </tr> <tr> <td>GLM-5</td> <td>5</td> <td>140</td> <td>Matches Kimi at similar price; open weights</td> </tr> <tr> <td>DeepSeek V3.2</td> <td>7</td> <td>23</td> <td>Tier 1 performance at 37× less than Opus</td> </tr> <tr> <td>Grok 4.1 Fast</td> <td>11</td> <td>20</td> <td>Cheapest Tier 1 model; high variance though (σ = 0.143)</td> </tr> <tr> <td>MiniMax M2.5</td> <td>12</td> <td>50</td> <td>Solid mid-tier at 17× less than Opus</td> </tr> <tr> <td>Gemini 3 Pro</td> <td>3</td> <td>370</td> <td>If you want a top-3 model, it's the cheapest of the three</td> </tr> </tbody> </table> <p>&lt;h3 id=coding-and-agentic-coding-opus-stands-out&gt;Coding and agentic coding: Opus stands out&lt;/h3&gt;&lt;p&gt;The coding ranking, drawn from seven benchmarks including demanding agentic evaluations, tells a more subtle story.&lt;/p&gt;</p> <table> <thead> <tr> <th>Rank</th> <th>Model</th> <th>Avg Pctl</th> <th>Std Dev</th> <th># Benchmarks</th> <th>Cost/1k</th> </tr> </thead> <tbody> <tr> <td>1</td> <td>opus46</td> <td>0.017</td> <td>0.016</td> <td>7</td> <td>850</td> </tr> <tr> <td>2</td> <td>gpt-codex53</td> <td>0.038</td> <td>0.027</td> <td>6</td> <td>470</td> </tr> <tr> <td>3</td> <td>gemini-pro3</td> <td>0.043</td> <td>0.033</td> <td>7</td> <td>370</td> </tr> <tr> <td>4</td> <td>gpt52</td> <td>0.065</td> <td>0.056</td> <td>7</td> <td>470</td> </tr> <tr> <td>5</td> <td>gpt-pro52</td> <td>0.096</td> <td>0.132</td> <td>3</td> <td>5600</td> </tr> <tr> <td>6</td> <td>gemini-flash3</td> <td>0.103</td> <td>0.089</td> <td>7</td> <td>100</td> </tr> <tr> <td>7</td> <td>sonnet45</td> <td>0.113</td> <td>0.135</td> <td>7</td> <td>500</td> </tr> <tr> <td>8</td> <td>kimi25</td> <td>0.128</td> <td>0.136</td> <td>7</td> <td>120</td> </tr> <tr> <td>9</td> <td>glm5</td> <td>0.130</td> <td>0.201</td> <td>7</td> <td>140</td> </tr> <tr> <td>10</td> <td>minimax25</td> <td>0.155</td> <td>0.162</td> <td>6</td> <td>50</td> </tr> <tr> <td>11</td> <td>deepseek32</td> <td>0.237</td> <td>0.222</td> <td>7</td> <td>23</td> </tr> <tr> <td>12</td> <td>haiku45</td> <td>0.240</td> <td>0.204</td> <td>5</td> <td>170</td> </tr> <tr> <td>13</td> <td>grok</td> <td>0.282</td> <td>0.288</td> <td>5</td> <td>600</td> </tr> <tr> <td>14</td> <td>grok-fast41</td> <td>0.301</td> <td>0.300</td> <td>5</td> <td>20</td> </tr> <tr> <td>15</td> <td>llama-maverick</td> <td>0.306</td> <td>0.257</td> <td>4</td> <td>55</td> </tr> <tr> <td>16</td> <td>gpt-instant52</td> <td>0.321</td> <td>0.292</td> <td>5</td> <td>470</td> </tr> <tr> <td>17</td> <td>gpt-oss-120b</td> <td>0.344</td> <td>0.282</td> <td>5</td> <td>40</td> </tr> <tr> <td>18</td> <td>mistral</td> <td>0.347</td> <td>N/A</td> <td>1</td> <td>400</td> </tr> <tr> <td>19</td> <td>qwen3-80b</td> <td>0.350</td> <td>0.346</td> <td>3</td> <td>N/A</td> </tr> <tr> <td>20</td> <td>qwen3-235b</td> <td>0.375</td> <td>0.262</td> <td>6</td> <td>300</td> </tr> <tr> <td>21</td> <td>qwen3-coder-30</td> <td>0.460</td> <td>0.348</td> <td>5</td> <td>50</td> </tr> <tr> <td>22</td> <td>gpt-oss-20b</td> <td>0.489</td> <td>0.355</td> <td>3</td> <td>15</td> </tr> </tbody> </table> <p><img src="https://bear-images.sfo2.cdn.digitaloceanspaces.com/residuals/ranks_coding.webp" alt="ranks_coding"> <strong>Figure 2:</strong> <em>Coding and agentic coding ranking</em>. Notation is the same as in Figure 1.</p> <p><em>Claude Opus 4.6 is the clear coding champion</em>. It consistently lands in the top 3% across all seven coding benchmarks—a level of reliability no other model matches. GPT-5.3 Codex and Gemini 3 Pro are also strong, but notice how Opus's standard deviation is roughly half of theirs.</p> <p><em>Agentic benchmarks expose inconsistencies</em>. Several models that score well on traditional coding tasks (function calling, SWE-bench Verified) stumble on agentic evaluations like Terminal-Bench 2.0 and SWE-Bench Pro, where the model must navigate multi-step tool use and environment interaction. Several models (e.g. GLM-5 with <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>σ</mi><mo>=</mo><mn>0.2</mn></mrow></math>, Grok with <math xmlns="http://www.w3.org/1998/Math/MathML" display="inline"><mrow><mi>σ</mi><mo>≈</mo><mn>0.3</mn></mrow></math>) show particularly high variance—their best and worst benchmark performances differ by an order of magnitude in percentile terms. Pure code generation ability doesn't reliably predict agentic coding performance.</p> <p><strong>The “coding” value picks:</strong></p> <table> <thead> <tr> <th>Model</th> <th>Coding rank</th> <th>Cost</th> <th>Note</th> </tr> </thead> <tbody> <tr> <td>Kimi K2.5</td> <td>8</td> <td>120</td> <td>Consistently solid across all 7 benchmarks</td> </tr> <tr> <td>MiniMax M2.5</td> <td>10</td> <td>50</td> <td>17× cheaper than Opus</td> </tr> <tr> <td>Gemini 3 Flash</td> <td>6</td> <td>100</td> <td></td> </tr> <tr> <td>DeepSeek V3.2</td> <td>11</td> <td>23</td> <td>Competitive at a tiny fraction of the cost</td> </tr> </tbody> </table> <p>&lt;h3 id=other-observations&gt;Other observations&lt;/h3&gt;&lt;p&gt;<strong>Consistency across the two rankings.</strong> Claude Opus 4.6 is #1 in both. GPT-5.2 is #2 in general but #4 in coding (the specialized Codex variant takes #2 there). Gemini 3 Pro is #3 in both—arguably the most balanced model overall. The biggest mover is Grok 4, which lands at #6 in general reasoning but drops to #13 in coding, suggesting its strengths are more on the reasoning side than the code generation side.&lt;/p&gt;</p> <p><strong>The Chinese open-weights models are the story of this analysis.</strong> Four models—Kimi K2.5, GLM-5, DeepSeek V3.2, and MiniMax M2.5—land in the top 12 for <em>both</em> general reasoning and coding. Their average cost is approximately 83 credits per 1k tokens, roughly <em>10× less</em> than the Big Three models (Opus, GPT-5.2, Gemini 3 Pro) average of ~560 credits. They are especially compelling if you can tolerate somewhat higher variance—they're brilliant on some benchmarks, merely good on others—compared to the more uniform excellence of the priciest proprietary models.</p> <p>The notable exception among Chinese models is Qwen3: the 235B variant ranks 14th in general and 20th in coding; the smaller Coder 30B model is near the bottom of both lists.</p> <p><strong>Cost scales much faster than quality.</strong> Going from DeepSeek V3.2 (23 credits) to Opus 4.6 (850 credits) is a 37× price increase that buys roughly a 4× improvement in average general reasoning percentile (0.116 → 0.027). Spending further to reach GPT-5.2 Pro (5,600 credits)—another 6.6× cost increase—actually makes things <em>worse</em> on average (0.209 percentile). The diminishing returns curve is brutally steep at the high end.</p> <h2 id="conclusions">Conclusions</h2> <ul> <li>If you need the absolute best coding reliability and can afford it, Claude Opus 4.6 earns its price.</li> <li>For general reasoning, you have several options: Kimi K2.5 or GLM-5 deliver Tier 1 performance at a fraction of the cost.</li> <li>For coding and agentic coding (e.g. <a href="https://opencode.ai" rel="external nofollow noopener" target="_blank">opencode</a>), there are many options with great value: Gemini Flash 3, Kimi K2.5, GLM-5 and MiniMax M2.5.</li> <li>If budget is the primary concern, DeepSeek V3.2 at 23 credits per 1k tokens is the standout value of this generation—statistically indistinguishable from the leaders on general reasoning, and still in the top half for coding, at roughly the cost of a rounding error.</li> </ul> <hr> <h2 id="reproducibility">Reproducibility</h2> <p>All code, data files, and full methodology details are <a href="https://github.com/rsnemmen/rank-clippies" rel="external nofollow noopener" target="_blank">here</a>. You can reproduce the rankings with</p> <div class="highlight"><pre><span></span><span class="c1"># General ranking</span>
<span class="n">python</span> <span class="n">rank_models</span><span class="o">.</span><span class="n">py</span> <span class="n">ranks_general</span><span class="o">.</span><span class="n">txt</span> <span class="o">--</span><span class="n">plot</span>
<span class="c1"># Coding ranking</span>
<span class="n">python</span> <span class="n">rank_models</span><span class="o">.</span><span class="n">py</span> <span class="n">ranks_coding</span><span class="o">.</span><span class="n">txt</span> <span class="o">--</span><span class="n">plot</span>
</pre></div> <p>These results correspond to <a href="https://github.com/rsnemmen/rank-clippies/commit/423486a1e0328d58d2aeaebabc2405835908431e" rel="external nofollow noopener" target="_blank">commit <code>423486a1e0328</code></a>.</p> <h2 id="changelog">Changelog</h2> <p>Feb 13 2026: Light introduction revision (phrasing).<br> Feb 14 2026: Minor formatting fixes.<br> Feb 15: I realized that the https://artificialanalysis.ai index shows only a selection of models in their plots, so their ranking is a bit misleading. I fixed this and as a result the average rankings and tiers were slightly impacted. <em>Conclusions are unchanged</em>. For the latest plots, please see <a href="https://github.com/rsnemmen/rank-clippies/blob/main/ranks_general.png" rel="external nofollow noopener" target="_blank">this for general intelligence</a> and <a href="https://github.com/rsnemmen/rank-clippies/blob/main/ranks_coding.png" rel="external nofollow noopener" target="_blank">this for coding/agentic coding performance</a>.</p> </body></html>